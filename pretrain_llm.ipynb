{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(\n",
    "            b, num_tokens, self.num_heads, self.head_dim\n",
    "            )\n",
    "        \n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "        GELU(),\n",
    "        nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "        d_in=cfg[\"emb_dim\"],\n",
    "        d_out=cfg[\"emb_dim\"],\n",
    "        context_length=cfg[\"context_length\"],\n",
    "        num_heads=cfg[\"n_heads\"],\n",
    "        dropout=cfg[\"drop_rate\"],\n",
    "        qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        \n",
    "        pos_embeds = self.pos_emb(\n",
    "        torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "def generate_text_simple(model, idx,\n",
    "                         max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:,-1,:]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583584255642\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Training an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                       optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.824, Val loss 9.935\n",
      "Ep 1 (Step 000005): Train loss 8.066, Val loss 8.337\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.621, Val loss 7.049\n",
      "Ep 2 (Step 000015): Train loss 6.044, Val loss 6.598\n",
      "Every effort moves you, and,, and,, and,,,, and,.                                   \n",
      "Ep 3 (Step 000020): Train loss 5.549, Val loss 6.486\n",
      "Ep 3 (Step 000025): Train loss 5.473, Val loss 6.399\n",
      "Every effort moves you, and to the! to the of the to the picture.                                     \n",
      "Ep 4 (Step 000030): Train loss 5.052, Val loss 6.305\n",
      "Ep 4 (Step 000035): Train loss 4.762, Val loss 6.305\n",
      "Every effort moves you, and I had been the of the picture to the picture. Gisburn, I had the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the\n",
      "Ep 5 (Step 000040): Train loss 4.172, Val loss 6.201\n",
      "Every effort moves you know the \"Oh, and I felt to me--as of the fact of the picture--and it the fact of the sketch of the fact of the fact of the picture.             \n",
      "Ep 6 (Step 000045): Train loss 3.776, Val loss 6.146\n",
      "Ep 6 (Step 000050): Train loss 3.208, Val loss 6.117\n",
      "Every effort moves you know the                                                \n",
      "Ep 7 (Step 000055): Train loss 3.165, Val loss 6.188\n",
      "Ep 7 (Step 000060): Train loss 2.425, Val loss 6.139\n",
      "Every effort moves you know the picture to see the picture.  \"I had the last word.           \"I he was his pictures-c his pictures--the his pictures, and down the room, I was\n",
      "Ep 8 (Step 000065): Train loss 1.951, Val loss 6.148\n",
      "Ep 8 (Step 000070): Train loss 1.629, Val loss 6.215\n",
      "Every effort moves you?\"  \"Yes--I glanced after him, and uncertain. \"Oh, he was a year after Jack's resolve had been. Gisburn's head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.269, Val loss 6.228\n",
      "Ep 9 (Step 000080): Train loss 0.971, Val loss 6.264\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"Once, I was, the fact, and that, I felt him back his head to look up at the honour being _mine_--because he had always _\n",
      "Ep 10 (Step 000085): Train loss 0.713, Val loss 6.350\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "    epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Decoding strategies to control randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 Modifying the text generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size,\n",
    "             temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know I found to do the picture for nothing--I\n",
      "\"I enough\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Loading and saving model weights in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_23424\\936198766.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "\"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_23424\\4170505291.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
